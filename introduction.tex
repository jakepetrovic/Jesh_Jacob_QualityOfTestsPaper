%!TEX root=qsic2014.tex
% mainfile: qsic2014.tex

\todo{Make sure we use a consistent terminology throughout the paper -- this
terminology should also align with the title: 

test cases vs tests vs test suite

developer-written vs. manually-written test cases}

\section{Introduction}

Since the release of low quality software has serious economic and social
consequences~\cite{tassey2002}, software testing is commonly used throughout the
software development process to identify defects and establish confidence in
program correctness. In fact, Kochhar et al.'s recent investigation of over
20,000 GitHub projects revealed that over 61\% of them include test
cases~\cite{kochhar2013}.  Even though testing is both valuable and prevalent,
the creation, execution, and maintenance of tests is one of the most expensive
aspects in software development --- often comprising more than 25\% of the total
\mbox{development costs~\cite{vizard2013}.}

Traditionally, test cases are manually written by software developers and/or
members of a quality assurance team.  Manual creation of effective test cases
requires human effort in terms of thought and time.  This effort can be
prohibitive for large projects, which often necessitate testing the
most~\cite{kochhar2013}.  As an alternative to manually writing test cases, many
automatic test case generation tools
(e.g.,~\cite{fraser:2011:eat:2025113.2025179,pacheco2007feedback,csallner2004})
are now available to help developers test their software.  However, the
effectiveness of the test cases generated by these tools
varies~\cite{bacchelli2008,fraser2013c,fraser2013a}, and it is unclear how test
cases generated by automated tools compare to those that are manually developed.

When given sufficient time and resources, developers can often manually
implement test cases that exercise the majority of the application's features
and cover a large portion of its source code.  Yet, the focus of the test cases
may vary depending on the software developer, the goals of the project, and/or
the standards of the company or open-source community~\cite{kochhar2013}.  Some
developers may write test cases with the goal of increasing code coverage ---
particularly in terms of statements or branches.  Other developers may focus
test cases on code that is likely to fail, most commonly executed, or
``important'' according to other standards~\cite{mockus2009}. 
Although there are tutorials that explain how to write unit tests
(e.g.,~\cite{vogella2013}), there is no well-established standard for writing
effective test cases, thus making the testing process even more challenging for
developers.  Besides, as the complexity and number of features of a program
increases, manually developing and maintaining test cases becomes more
expensive~\cite{clarke1998automated}.  

Automatic test generation requires far less human time and effort compared to
manually developing test cases. This time savings may, however, not be
worthwhile if the resulting test cases are poor in terms of code coverage or
fault-finding capability.  Moreover, automatically generated test cases may not
be as useful as developer-written ones if they are hard to understand and 
difficult to maintain.

When confronted with the wide variety of non-standardized manual testing
guidelines and complex automated test generation tools, software developers face
the challenge of defining a strategy for developing effective test cases.

Through an empirical study based on ten real-world open-source programs, this
paper characterizes and compares developer-written and automatically generated
test cases, providing useful insights for both researchers and practitioners.
%
\todo{Can we add more details about the provided insights?} 
%
Employing two state-of-the-art test generation tools (\evo and \codepro), this
paper examines and compares the complexity, efficiency, and effectiveness of
automatically generated and developer-written test cases. Additionally, the
paper points out the practical benefits and challenges associated with using
automatic test generation tools.

In summary, this paper's main contributions are:
\squishlist
\item A comparison of the characteristics of automatic test generation tools.

\item A comparison of complexity (size\todo{and ???}), 
      efficiency (execution time), and 
      effectiveness (code coverage and fault-finding capability) 
      of developer-written and automatically generated test cases.

\item A discussion of the benefits and drawbacks of using automatic test 
      generation tools.
\squishend


\section{Quality of Test Cases}

While goals, company policy, and/or community standards may influence the
quality of manually implemented test suites, automatically generated ones are
similarly affected by both the choice of the tool and the configuration of the
tool's parameters.  Test suite quality is frequently measured based on the
amount of code covered and on the fault-finding ability of the test suite.  Code
coverage is a structure-based criterion that requires the exercising of certain
control structures and variables within
program~\cite{kapfhammer-testing-handbook}. A fault-based test adequacy
criterion, on the other hand, attempts to ensure that the program does not
contain the types of faults that developers inadvertently introduce into
software systems~\cite{demillo1978hints}.  

A common type of test quality evaluation, in both industrial and academic work,
leverages structurally-based criteria that require the execution of a statement
or branch in a program~\cite{weyuker1988evaluation}.  Alternatively, mutation
testing is a fault-based technique that measures the fault-finding effectiveness
of test suites on the basis of induced faults~\cite{demillo1978hints,
hamlet1977testing}.  Originally proposed by DeMillo et
al.~\cite{demillo1978hints}, mutation testing is a well-known technique that
evaluates the quality of tests by seeding faults into the program under test;
each altered version containing a seeded fault is called a mutant. A test is
said to kill a mutant if the output of that test varies when executed against
the mutant instead of the original program; the mutation score is the ratio of
killed mutants to generated mutants. Mutants of the original program are
obtained by applying mutation operators. For example, a conditional statement
\texttt{if (a < b)} results in multiple mutants by replacing the relational
operator \texttt{<} with valid alternatives such as \texttt{<=} or \texttt{!=}.
Prior studies have used mutation adequacy to experimentally gauge the
effectiveness of different testing
strategies~\cite{andrews2005mutation,andrews2006,do2006,just2014}.  

\section{Generation of Test Cases}

Instead of, or in addition to, manually implementing test cases without rigorous
guidelines, developers may employ automatic test case generation tools that
could reduce the time and cost associated with testing while also improving code
coverage.  Many automatic test suite generators currently exist.  Some tools,
like the MoreUnit plugin for Eclipse \cite{moreunit}, generate test case stubs
for the method that is currently highlighted by the cursor.  Alternatively,
tools such as \codepro~\cite{CodePro1},
\evo~\cite{fraser:2011:eat:2025113.2025179}, \jcrasher~\cite{csallner2004},
\palus~\cite{zhang:2011:pha:1985793.1986036},
\randoop~\cite{pacheco2007feedback}, and
\testera~\cite{marinov:2001:tnf:872023.872551} can generate complete test suites
that need no modification prior to execution.

Automatic test case generation tools use both deterministic (e.g., hard-coded
rules and regular expressions) and learning-based (e.g., randomized or
evolutionary) algorithms to produce test cases based on particular strategies,
thereby avoiding the subjectivity and wide variation in styles commonly found in
manually generated tests.  Test suite generators also have the potential to
significantly reduce the amount of human time and effort required of the
developer to create the test suite.  



