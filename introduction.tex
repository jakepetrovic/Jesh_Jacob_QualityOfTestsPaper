%!TEX root=qsic2014.tex
% mainfile: qsic2014.tex

\section{Introduction}

Since the release of low quality software has serious economic and social
consequences~\cite{tassey2002}, software testing is commonly used throughout the
software development process to identify defects and establish confidence in
program correctness. In fact, Kochhar et al.'s recent investigation of over
20,000 GitHub projects revealed that over 61\% of them include test
cases~\cite{kochhar2013}.  Even though testing is both valuable and prevalent,
the creation, execution, and maintenance of tests is one of the most expensive
aspects in software development --- often comprising more than 25\% of the total
\mbox{development costs~\cite{vizard2013}.}

Traditionally, test cases are manually written by software developers and/or
members of a quality assurance team.  Manual creation of effective test cases
requires human effort in terms of thought and time.  This effort can be
prohibitive for large projects, which often necessitate testing the
most~\cite{kochhar2013}.  As an alternative to manually writing test cases, many
automatic test case generation tools
(e.g.,~\cite{fraser:2011:eat:2025113.2025179,pacheco2007feedback,csallner2004})
are now available to help developers test their software.  However, the
effectiveness of the test cases generated by these tools
varies~\cite{bacchelli2008,fraser2013c,fraser2013a}, and it is unclear how test
cases generated by automated tools compare to those that are manually developed.

When given sufficient time and resources, developers can often manually
implement test cases that exercise the majority of the application's features
and cover a large portion of its source code.  Yet, the focus of the test cases
may vary depending on the software developer, the goals of the project, and/or
the standards of the company or open-source community~\cite{kochhar2013}.  Some
developers may write test cases with the goal of increasing code coverage ---
particularly in terms of statements or branches.  Other developers may focus
test cases on code that is likely to fail, most commonly executed, or
``important'' according to other standards~\cite{mockus2009}. 
Although there are tutorials that explain how to write unit tests
(e.g.,~\cite{vogella2013}), there is no well-established standard for writing
effective test cases, thus making the testing process even more challenging for
developers.  Besides, as the complexity and number of features of a program
increases, manually developing and maintaining test cases becomes more
expensive~\cite{clarke1998automated}.  

Automatic test generation requires far less human time and effort compared to
manually developing test cases. This time savings may, however, not be
worthwhile if the resulting test cases are poor in terms of code coverage or
fault-finding capability.  Moreover, automatically generated test cases may not
be as useful as developer-written ones if they are hard to understand and 
difficult to maintain.

When confronted with the wide variety of non-standardized manual testing
guidelines and complex automated test generation tools, software developers face
the challenge of defining a strategy for developing effective test cases.

Through an empirical study based on ten real-world open-source programs, this
paper characterizes and compares developer-written and automatically generated
test cases, providing useful insights for both researchers and practitioners.
%
\todo{Can we add more details about the provided insights?} 
%
Employing two state-of-the-art test generation tools (\evo and \codepro), this
paper examines and compares the complexity, efficiency, and effectiveness of
automatically generated and developer-written test cases. Additionally, the
paper points out the practical benefits and challenges associated with using
automatic test generation tools.

In summary, this paper's main contributions are:
\squishlist
\item A comparison of the characteristics of automatic test generation tools.

\item A comparison of complexity (size\todo{and ???}), 
      efficiency (execution time), and 
      effectiveness (code coverage and fault-finding capability) 
      of developer-written and automatically generated test cases.

\item A discussion of the benefits and drawbacks of using automatic test 
      generation tools.
\squishend


