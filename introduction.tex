\section{Introduction}
%----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
In Software Engineering, testing the code is one of the most important parts of the development cycle. Often the quality of the tests dictate the quality of the software. (Need Reference) Test suites are created through a number of different methods. In most instances the test engineer will manually write the test cases. However, manual creation of test cases requires large amounts of time and effort. Although this gives developers the control over the quality of the tests, the cost of the time and effort required to produce these test suites can be too high to implement on established projects. In recent years, researchers have looked into a new way to reduce the cost and automate this process for developers. A new alternative has arisen as a potential solution to this issue -- auto-generated test suites. 

%Manual Generated Tests -  benefits vs drawbacks
Manually generated test suites allow developers to spend more time and effort on improving the quality of a test to insure it captures the full depth of the code. With a focus on the quality of tests, the work needed to generate a high coverage with a large number of test cases becomes expensive. As programs become more complex, so do the test suites that test them. As the program complexity rises, manually written test cases can become expensive and labor intensive when applied to highly complex features .~\cite{clarke1998automated}.

Additionally, there is no standard for writing test cases. This can lead to inconsistent and subjective methods of writing test cases. The cost of time and effort put into writing test cases leaves less resources for developers to invest into increasing the coverage of the test suite.

%Talk about auto generated test suites - benefits vs drawbacks
Auto-generated test suites have one goal in mind: reduce the cost of time and effort to write tests. The test suites themselves are generated from many different automated test suite generators ~\cite{Fraser:2011:EAT:2025113.2025179, Zhang:2011:PHA:1985793.1986036, Marinov:2001:TNF:872023.872551}. These programs use a consistent standards to produce test cases, removing the subjectivity and styles one may find in manually generated tests. Automated test generation also reduces the time and effort of creating large amounts of test cases to increase the breadth of code coverage. This allows test suites to be built faster without the effort of a developer. However, with the developer removed from the test case generation, the quality may be lost in the creation of the test cases. If the quality of test cases are shallow, then certain special cases may be missed, despite a large code coverage.

%Compare Automated vs Manual
Every developer's desire would be to have a faster, consistent, and cost effective way of testing their code. Most importantly though, an assurance that the tests cases still maintain a high quality received from manually written tests. The two choices developers have in testing is either manual or auto-generated test cases. When comparing automated and manual testing, automated tests are good at breadth but much less at depth ~\cite{4076909}.The question that engineers must ask then is whether or not the auto-generated test suites produce the same quality of test cases as those written manually by a test engineer. 

The quality of a test suite can be measured by its depth and breadth of coverage. Although the breadth, measured in terms of coverage may be extensive, more intricate and complicated test cases may not be covered by many available programs. It becomes difficult to determine what metrics can be used to establish the true definition of ``depth" and ``breadth". Mutation scores are traditionally used to assess the quality of test suites by inserting artificial defects (mutations) into programs ~\cite{Fraser:2010:MGU:1831708.1831728}. Based on the mutation score given by a mutation test suite, testers will determine the quality of the tests covering their code. However, when comparing automated and manually written tests, researchers need additional metrics to compare the two processes. For instance, what is the difference in creation time and execution time for an auto-generated versus a manually written test suite? If test creation takes less time, but produces poor quality test cases, then the cost of using these tools in terms of quality may outweigh the benefits received in time. If the automated generated tests has higher code coverage but the execution time is long, then the cost of time may be too high. 

Developers must also consider code coverage as producing more tests may increase the overall code coverage. Yet there is a possibility that more tests may be produced, but the coverage is small because the tests are redundant. In evaluating the time to create tests, time to execute tests, code coverage, code complexity, and mutation score, we can compare the quality, time, and effort between manual and automated test suites.

This paper observes whether the selected automated tools maintain the needed quality and coverage of manually written test suites. First impressions of auto-generated tests may leave more to be desired in their simplistic and current underdeveloped state. However, there have been few tests to actually determine the usefulness of automated tools. This research attempts to answer questions lingering over the differences in modern automated tools and traditional manual testing.

Measuring with mutation score, code coverage, complexity, and time to generate the test suite, this paper addresses how auto-generated tests differ from manually generated test cases, and if relying on these tools can retain the standard of quality that manually written test suites produce. Furthermore, the paperl compares metrics acquired from auto-generated tests to manually written tests to determine if auto-generated tests provide any advantages over to manually written test suites.

%main contribution
The main contribution of this paper is to reveal the differences and techniques used among different automated test suites. With these results, this paper remarks the benefits and drawbacks may occur when relying on automated generated test suites. This paper evaluates Evosuite and CodePro, two free automated-test tools that will generate the JUnit test suites. In Section~\ref{sec:background}, the paper first establishes the fundamentals of automated test generation, mutation analysis, and genetic algorithms. In Section~\ref{sec:evaluation}, the paper describes process for the evaluation each of these programs in comparison to manually written test suites, and in comparison together. The paper compares the test suites by mutation score, code coverage, complexity, and the time it takes to create the test suite. In Section~\ref{sec:related_work}, the paper compares other related work and research to this study. Finally, in Section~\ref{sec:conclusion}, the paper concludes the results and discusses future work possibilities.