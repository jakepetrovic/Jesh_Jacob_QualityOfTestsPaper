%!TEX root=qsic2014.tex
% mainfile: qsic2014.tex

\section{Related Work} \label{sec:related_work}

Since this paper focuses on empirically comparing manually implemented and automatically generated test suites, it is most directly related to Bacchelli et al.'s evaluation of the effectiveness of manual and automated test data generation~\cite{bacchelli2008}. However, there are several distinctions between our paper and this prior work. While the experiments in this paper focus on test suites for ten case study applications, Bacchelli et al.\ only consider select classes that implement data structures like {\tt LRUHashtable}. In contrast to the use of manual tests that were implemented by real-world open-source software developers, the manually-coded test cases in Bacchelli et al.'s were created by the authors themselves.  Moreover, Bacchelli et al.\ use Randoop~\cite{pacheco2007feedback} and JCrasher~\cite{csallner2004}, instead of picking \textsc{EvoSuite}, the current state-of-the-art test data generation tool~\cite{fraser2013a}. Finally, it is important to note that even though Bacchelli et al.\ report coverage and mutation scores, they neither investigate the correlation between coverage and mutation nor perform a comprehensive statistical analysis of the results. In addition, it is possible to make similar comparisons between our paper and the experimental work of Assylbekov et al.~\cite{assylbekov2013}.

The design of the our paper's experiments is informed, in part, by the experimental design and results presented by Inozemtseva and Holmes~\cite{inozemtseva2014}. Like our paper, this prior work also uses real-world programs to empirically investigate the relationship between code coverage and mutation score. However, the primary intent of our work is different than that of Inozemtseva and Holmes: while they investigate the effectiveness correlation for the test suites that come with programs, we consider both manually and automatically generated tests. Moreover, it is important to note that while their paper uses PIT~\cite{pit2014} for fault seeding, our experiments use MAJOR---the only mutation testing tool whose mutants are currently known to be statistically similar to real-world faults~\cite{just2014}.

Our paper's experiments are also partially influenced by the design and results reported on by Gopinath et al.~\cite{gopinath2014}.  This paper is similar to ours because it also investigates the correlation between a test suite's coverage and mutation score.  In addition, Gopinath et al.\ use both manually and automatically generated test suites for more programs than we do; yet, since our experimentation framework is easy to apply to new programs and our presented results demonstrate promise, we will scale our study to Gopinath et al.'s level in future work. Moreover, even though \textsc{EvoSuite} automatically generates better test suites than Randoop~\cite{fraser2013a}---thus motivating its use in our experiments---Gopinath et al.\ use Randoop to create test suites.  In addition, while Gopinath et al.\ employ PIT to seed faults into their case study applications, we decided to use MAJOR since recent results indicate that this tool's faults are a valid substitute for real faults~\cite{just2014}. 

Ultimately, the results from Inozemtseva and Holmes \cite{inozemtseva2014} and Gopinath et al.~\cite{gopinath2014}, in conjunction with those in this paper, present a complementary understanding of the effectiveness of automatically and manually generated test suites. It is also important to remark that, while Just et al.~\cite{just2014} are the first to establish a statistical correlation between a test suite's mutation score and its effectiveness at detecting real-world faults, the purpose of that work is not to develop a full-featured understanding of the quality characteristics of automatically and manually generated test suites---the aim of our paper.

% There are few studies comparing the quality of manual and automated test suites. In one study researchers found that
% using automated test generation tools did not improve the ability to test the software~\cite{fraser2013c}. However, the
% study does not measure the quality of manual and automated generated test suites directly.  Furthermore, only Evosuite
% is used to evaluate the mutations and generate the tests.  This paper expands upon the question of the usefulness of
% automated generated test suites, and if certain systems in generating test suites work better than others.

It is important to note that all of the aforementioned related work focuses of the empirical and technical aspects of test suite effectiveness that are not related to human-centric issues.  In part, our paper was motivated to investigate the complexity and understandability of automatically and manually generated test suites by Fraser et al.'s empirical results revealing that automatically generated tests do not always help software testers find more defects in a program~\cite{fraser2013c}.  While Fraser et al.\ also consider coverage and mutation scores for automatically and manually generated test suites, they only use one automated test suite generator, \textsc{EvoSuite}, while we additionally consider tests created with a deterministic tool called CodePro.

In contrast to our focus on manually and automatically generated test suites for real-world open-source applications, other related work has considered coverage and mutation scores for test suites written by students. For instance, Aaltonen et al.\ observe that automated grading programs may reward students for high-coverage test suites that actually have poor defect-revealing potential~\cite{aaltonen:2010:mav:1869542.1869567}, concluding that a combination of coverage and mutation scores may be the best way to give students accurate feedback on the quality of their test cases.  In addition, Shams and Edwards experimentally observe that mutation scores are lower than coverage values for student-implemented test suites~\cite{shams2013}---a result that corresponds to what we found for open-source programs.

% Another paper compares measuring quality between faults detection and mutation scores~\cite{just2014}. The research
% statistically proves that mutation score can be used as a test of quality of test suites in place of fault detections.
% The paper uses automated test suites to validate this research, and unlike this paper's research, the author's do not
% compare mutation score, branch coverage, and cyclomatic complexity as joint measurements of quality. 

There is research comparing multiple mutation analysis tools~\cite{ComparingAutomatedMutationTools:2013}. This paper differs from this research in specifically comparing mutation analysis tools for Java. MAJOR was recommended as one one of the premier tools, and was thus used for the evaluation.
