%!TEX root=qsic2014.tex
% mainfile: qsic2014.tex

\section{Related Work} \label{sec:related_work}

Since this paper focuses on empirically comparing manually implemented and automatically generated test suites, it is most directly related to Bacchelli et al.'s evaluation of the effectiveness of manual and automated test data generation~\cite{bacchelli2008}. However, there are several distinctions between our paper and this prior work. While the experiments in this paper focus on test suites for ten case study applications, Bacchelli et al.\ only consider select classes that implement data structures like {\tt LRUHashtable}. In contrast to the use of manual tests that were implemented by real-world open-source software developers, the manually-coded test cases in Bacchelli et al.'s were created by the authors themselves.  Moreover, Bacchelli et al.\ use Randoop~\cite{pacheco2007feedback} and JCrasher~\cite{csallner2004}, instead of picking EvoSuite, the current state-of-the-art test data generation tool~\cite{fraser2013a}. Finally, it is important to note that even though Bacchelli et al.\ report coverage and mutation scores, they do neither investigate the correlation between coverage and mutation nor perform a comprehensive statistical analysis of the results.

% There are few studies comparing the quality of manual and automated test suites. In one study researchers found that
% using automated test generation tools did not improve the ability to test the software~\cite{fraser2013c}. However, the
% study does not measure the quality of manual and automated generated test suites directly.  Furthermore, only Evosuite
% is used to evaluate the mutations and generate the tests.  This paper expands upon the question of the usefulness of
% automated generated test suites, and if certain systems in generating test suites work better than others.

Another paper compares measuring quality between faults detection and mutation scores~\cite{just2014}. The research statistically proves that mutation score can be used as a test of quality of test suites in place of fault detections.  The paper uses automated test suites to validate this research, and unlike this paper's research, the author's do not compare mutation score, branch coverage, and cyclomatic complexity as joint measurements of quality. 

There is research comparing multiple mutation analysis tools~\cite{ComparingAutomatedMutationTools:2013}. This paper differs from this research in specifically comparing mutation analysis tools for Java. MAJOR was recommended as one one of the premier tools, and was thus used for the evaluation.
