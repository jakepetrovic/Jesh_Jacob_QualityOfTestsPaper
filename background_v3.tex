%!TEX root=qsic2014.tex
% mainfile: qsic2014.tex

\section{Test Case Generation Techniques}
\label{sec:background}

This section discusses the processes of writing test cases manually and using automatic test case generation tools.  We also describe CodePro and \textsc{EvoSuite}, the automatic test case generation tools that are empirically studied in this paper.

\subsection{Manually Written Tests}

Test suites are most often written manually, either by the developers themselves or through a quality assurance team.  While companies may have their own standards and goals that are followed when writing test cases---such as high levels of statement or branch coverage (e.g.~\cite{DO-178B, IEC61508})---no well-established patterns exist to help standardize test writing practice throughout the software development industry. Thus, the methods and styles of writing individual tests, fulfillment of coverage and fault-finding goals, and the ordering of test suites are often left to industry requirements or personal preference.  

\subsection{Automatically Generated Tests}
Due to the high cost and inconsistencies introduced when developing test suites by hand, automatic test suite generation research is on the rise.  In the past, the writing of test cases was left as an afterthought, and their generation was the responsibility of a separate quality assurance team rather than the developer.  This led to a disconnect between the code and the tests.  In recent years, however, there has been a move towards a more involved test development system in tandem with the the development process ~\cite{Gelperin:1988:GST:62959.62965}.  This movement includes focus on creating unit tests for code as its developed to ensure that code always passing tests thereby improving the quality of the code ~\cite{Canfora:2006:EAT:1159733.1159788}.  Although this improvement in test generation processes successfully increased the reliability of the code, the cost of time and effort to manually write high quality test cases increases as programs became more complex~\cite{clarke1998automated}. 

While many different techniques have been used to automatically generate tests, they can be divided into two key categories: Deterministic and Learning-Based.

\subsubsection{Deterministic}
Deterministic automatic test case generators generally analyze method parameters and basic paths to create unit tests.  The simplest of these tools statically analyze the basic source code paths alone and create skeletons of  needed tests.  For example, JUnitDoclet \cite{JUnitDoclet} uses Javadoc to parse the source code of the application classes. From the collected information, JUnitDoclet writes TestCases and TestSuites where there is a TestSuite for each Java package, a TestCase for each public, non-abstract class, and a skeleton test method for each public method. %The compiler will additionally guide the developer to challenging code segments such as classes that do not have a public constructor, classes that have no default constructors, and accessors for double or float values that need some epsilon when comparing two values.

While these test skeletons are helpful, more sophisticated tools have been developed that create fuller tests by taking the method parameters into consideration. CoView~\cite{CoView}, for example, is a commercial Eclipse plug-in tool that analyzes Java source code and calculates the number of data-driven and cyclomatic paths in a method. Each path is one that should be verified via a unit test. CoView then analyzes existing JUnit tests to determine which paths are being tested and which paths are not tested. This determination is made using instrumented byte code to determine path and branch coverage. CoView then creates missing JUnit test cases for the developer. The developer will have to modify parts of the tests such as the assertions, but the tool can help the developer by identifying the minimum number of unit tests that should be created given parameter options and paths.

Other tools are capable of generating fully executable tests that require no modification.  In this paper, CodePro Analytix is used.  CodePro~\cite{CodePro1} is an Eclipse plug-in tool with many powerful code analysis features and metrics.  Given an input class, the tool creates a corresponding test class complete with multiple test methods for each input class method. The tool analyzes each method and input argument with the goal of generating test cases that exercise each line of code using a combination of both static code analysis and by dynamically executing the code to be tested in order to observe the behavior of the code~\cite{CodePro2}.  CodePro was a Jolt Award finalist and has been analyzed in terms of the types of test cases it can write compared to other tools~\cite{xie2009}.  However, to the best of our knowledge, no work has compared the overall quality of the test cases it creates.

\subsubsection{Learning-Based}
Another set of automatic test case generation tools use learning algorithms to improve the overall quality of the generated test suites.  The two top-ranked tools in this area are Randoop and \textsc{EvoSuite}~\cite{fraser2013a}.  Randoop~\cite{pacheco2007feedback} automatically creates unit tests for Java classes in JUnit format using feedback-directed random test generation. This technique randomly generates sequences of methods and constructor invocations for the classes under test and uses the sequences to create tests. Randoop then executes the sequences it creates and uses the results of the execution to create more assertions, attempting to  avoid redundant and illegal inputs while guiding towards generation of tests that lead to new object states. 

\textsc{EvoSuite}~\cite{fraser:2011:eat:2025113.2025179} , which is used in this paper, ranked first in SBST 2013 Tool Competition~\cite{fraser2013a} and similarly uses a learning algorithm to generate a full, executable test suite.  The tool's evolutionary search approach evolves whole test suites with respect to both coverage and mutation scores.  Optimization with respect to a coverage criterion rather than individual coverage goals helps the algorithm to not be adversely influenced by difficulty of infeasibility of individual coverage goals.  Repeated mutation testing is used to produce a reduced set of assertions that maximizes the number of seeded defects in a class that are revealed by the test cases.

