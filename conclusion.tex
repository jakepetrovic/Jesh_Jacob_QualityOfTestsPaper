\section{Conclusions and Future Work}
\label{sec:conclusion}
%----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Developers have struggled to find a standard of measuring the quality of tests. Mutation testing and seeding faults has proven as one effective way to identify lower quality tests, and many developers consider high branch coverage a sufficient mark of quality for their test suites. The dilemma becomes a cost trade off between depth and breadth, two measurements that this paper uses to address quality. If mutation analysis results represent the depth of test suites, and branch coverage represents the breadth of test suites, the results of this research identifies trends between the automated tools and developers who write tests. Mutation scores and branch coverage correlate in a positive fashion between the two.

Complexity of the program appeared to also influence the mutation score and time to generate the test suite. In the case of the time needed to generate the test suite, a tool utilizing a genetic algorithm took much longer than a deterministic algorithm to generate test suite. However, the cost of time could be outweighed by the better quality received in the mutation scores. The genetic algorithm sustained a higher mutation score than even the manually written tests when the programs were larger. It may be that automated test tools cover larger programs better than manual tests do.

This paper also addresses new questions. First, what is the purpose of auto generating test suites? CodePro garnered poor scores for both mutation and branch coverage results. Although the quality suffered, the research revealed that the CodePro tests were modifiable. Perhaps some tools could be used assist developers in developing a test suite with a template, placing a focus on modifiable tests rather than a complete test suite to match the quality of manually written tests.

Secondly, what is the future of genetic algorithm based tools such as Evosuite? The tests may not be as modifiable as CodePro's, but the mutation scores in some cases matched and exceeded the mutation scores of manually written tests. If the cost of time and effort given from running Evosuite is less and will generate almost the same results for a large program, perhaps developers ought to consider using a tool like Evosuite.

Finally, the paper identifies the inconsistencies between manually generated tests. There is no way one can say with certainty that the goal of each of the developers for each of the projects was to achieve a high mutation score or high branch coverage. Perhaps developers only care to test one function of the code in depth, or rather they would seek to achieve a high branch coverage. Either way, the intentions of achieving the quality standards outlined in this paper may not be ideal for every developer. 

%GREG\KRISTEN - Will we need something like this?
In order to reproduce this study and further research in software testing, please reference our material on our web page:

 http://cs.uccs.edu/~kjustice/QSIC2014/