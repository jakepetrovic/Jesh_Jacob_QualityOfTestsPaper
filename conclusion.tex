\section{Conclusions and Future Work}
\label{sec:conclusion}
%----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
The creation, execution, and maintenance of tests is one of the most expensive aspects of developing software, but tools such as CodePro and \textsc{EvoSuite} can aide developers in reducing the cost of this process.  In this paper, the tradeoffs of using automatically generated tests versus manually generated tests are compared.  The time for test suite generation, number of tests produced, time of execution of the resulting test suites, and quality of the test suites are evaluated.  Quality is measured based on the branch coverage and mutation scores of the resulting test suites.

The results indicate that automated test cast generation tools can quickly generate more tests than are provided in manual test suites.  While CodePro creates an average of 5\% more test cases than \textsc{EvoSuite} and 16.4\% more than were written manually and \textsc{EvoSuite} produced, on average, 4\% more than were created manually, CodePro's test quality was low when both branch coverage and mutation score were considered.  There was little correlation between branch coverage and mutation score for these test suites overall.  \textsc{Evosuite}'s test suites, however, were positively correlated in quality between branch coverage and mutation score with an $R^2$ value of 0.4.  Manually generated tests demonstrated an even stronger correlation between branch coverage and mutation score with an $R^2$ value of 0.64.  

While some manually written tests were of higher quality overall in terms of branch coverage and mutation scores, the results indicate that more sophisticated, learning-based automated test generation tools such as \textsc{EvoSuite} can be used to produce test suites of similar quality on average compared to manual tests.  On average, manual tests covered 31.5\% of the branches while  \textsc{EvoSuite} covered 31.86\% of the branches. In terms of mutation score, \textsc{EvoSuite}'s generated tests had an average mutation score of 39.89\% compared to the average mutation score of 42.14\% for manually written tests.  Given the time reduction of using an automated tool compared to hand writing tests, these results are significant and encourage the use of automated tools for test production.  

In future work, we will increase the number and types of case study applications under consideration.  There are also a number of other tools including Randoop, T2, and DSC that we would like to include in our calculations.  These three automated test case generation tools have proven competitive in producing high quality tests.  Finally, we will consider other quality analysis measurements such as statement coverage and modified condition/decision coverage and tools such as PIT to back up our results and perform further analysis.  

%Developers have struggled to find a standard of measuring the quality of tests. Mutation testing and seeding faults has proven as one effective way to identify lower quality tests, and many developers consider high branch coverage a sufficient mark of quality for their test suites. The dilemma becomes a cost trade off between depth and breadth, two measurements that this paper uses to address quality. If mutation analysis results represent the depth of test suites, and branch coverage represents the breadth of test suites, the results of this research identifies trends between the automated tools and developers who write tests. Mutation scores and branch coverage correlate in a positive fashion between the two.

%Complexity of the program appeared to also influence the mutation score and time to generate the test suite. In the case of the time needed to generate the test suite, a tool utilizing a genetic algorithm took much longer than a deterministic algorithm to generate test suite. However, the cost of time could be outweighed by the better quality received in the mutation scores. The genetic algorithm sustained a higher mutation score than even the manually written tests when the programs were larger. It may be that automated test tools cover larger programs better than manual tests do.

%This paper also addresses new questions. First, what is the purpose of auto generating test suites? CodePro garnered poor scores for both mutation and branch coverage results. Although the quality suffered, the research revealed that the CodePro tests were modifiable. Perhaps some tools could be used assist developers in developing a test suite with a template, placing a focus on modifiable tests rather than a complete test suite to match the quality of manually written tests.

%Secondly, what is the future of learning based tools such as  \textsc{EvoSuite}? The tests may not be as modifiable as CodePro's, but the mutation scores in some cases matched and exceeded the mutation scores of manually written tests. If the cost of time and effort given from running  \textsc{EvoSuite} is less and will generate almost the same results for a large program, perhaps developers ought to consider using a tool like  \textsc{EvoSuite}.

%Finally, the paper identifies the inconsistencies between manually generated tests. There is no way one can say with certainty that the goal of each of the developers for each of the projects was to achieve a high mutation score or high branch coverage. Perhaps developers only care to test one function of the code in depth, or rather they would seek to achieve a high branch coverage. Either way, the intentions of achieving the quality standards outlined in this paper may not be ideal for every developer. 

For full result analyses and correlation summaries and more results from our evaluations, please reference our web page: \url{http://cs.uccs.edu/~kjustice/QSIC2014/.}

%GREG\KRISTEN - Will we need something like this?
%In order to reproduce this study and further research in software testing, please reference our material on our web page:

 %http://cs.uccs.edu/~kjustice/QSIC2014/