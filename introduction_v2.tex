%!TEX root=qsic2014.tex
% mainfile: qsic2014.tex

\section{Introduction}

% Software testing is a vital task throughout the software development lifecycle for helping to create quality software.

Since the release of low quality software has serious economic and social consequences~\cite{tassey2002}, software testing is commonly used throughout the software development lifecycle to identify defects and establish a confidence in program correctness. In fact, Kochhar et al.'s recent investigation of over 20,000 GitHub projects revealed that over 61\% of them include test cases~\cite{kochhar2013}.  However, even though testing is both valuable and prevalent, the creation, execution, and maintenance of tests is also one of the most expensive aspects of developing software, often comprising atleast 25\% of the total budget for an information technology department~\cite{vizard2013}.  

% Manual and automated testing, and the lack of clarity about the quality of test suites

Traditionally, test cases are written manually by software developers and/or members of a quality assurance team.  Unfortunately, manual creation of a high quality test suite requires substantial amounts of human thought, time, and effort.  Although manual creation gives developers control over what code they test and how thoroughly they test it, the time and effort required to produce these test suites can be prohibitively high in large projects that often necessitate the most test cases~\cite{kochhar2013}.  As an alternative to manually writing test cases, many automatic test case generation tools (e.g.,~\cite{fraser:2011:eat:2025113.2025179,pacheco2007feedback,csallner2004}) are now available to help developers test their software.  However, the quality of the test suites produced by these tools varies~\cite{bacchelli2008,fraser2013c,fraser2013a}, and it is unclear how the test suites of automatic test generation tools compare to those that are manually developed.

% Manual Generated Tests -  benefits vs drawbacks

When given sufficient time and resources, developers can often manually implement a high quality test suite that exercises the majority of the application's features and covers its source code.  Yet, the focus of the test cases may vary depending on the software developer, the goals of the project, and/or the standards of the company or open-source community~\cite{kochhar2013}.  Some developers may write tests with the goal of increasing the coverage of the code, particularly in terms of statements or branches.  Others may focus tests on code that is likely to fail, most commonly executed, or ``important'' according to other standards~\cite{mockus2009}. In any case, as the size and complexity of a program increases, manually writing test cases can become expensive, thus requiring additional human thought and labor when applied to highly complex features~\cite{clarke1998automated}.

% The lack of standards for test suites

% Talk about auto generated test suites - benefits vs drawbacks

Although there are tutorials that explain how to write JUnit test suites (e.g., \cite{vogella2013}), there is no well-established standard for writing test cases, thus making the testing process even more challenging for developers.  Instead of, or in addition to, manually implementing test cases without rigorous guidelines, developers may employ automatic test case generation tools that could reduce the time and cost associated with testing while also improving code coverage.  Many automatic test suite generators currently exist.  Some tools, like the MoreUnit plugin for Eclipse \cite{moreunit}, generate test case stubs for the method that is currently highlighted by the cursor.  Alternatively, tools such as CodePro AnalytiX~\cite{codepro1}, \textsc{EvoSuite}~\cite{fraser:2011:eat:2025113.2025179}, JCrasher~\cite{csallner2004}, Palus~\cite{zhang:2011:pha:1985793.1986036}, Randoop~\cite{pacheco2007feedback}, and TestEra~\cite{marinov:2001:tnf:872023.872551} can generate complete test suites that need no modification prior to execution.

% Compare Automated vs Manual

% This has to be connected to empirical studies that compare both automated and manual?

% While goals, company policy, and/or community standards may influence a developer's choice to test certain
% code segments, the ability of test suites to find bugs is not frequently analyzed.

Automatic test case generation tools use both deterministic (e.g., hard-coded rules and regular expressions) and
learning-based (e.g., randomized or evolutionary) algorithms to produce test cases based on particular strategies,
thereby avoiding the subjectivity and wide variation in styles commonly found in manually generated tests.  Test suite
generators also have the potential to significantly reduce the amount of human time and effort required of the developer
to create the test suite.  

% However, the quality of the test suites is also of vital concern.  

While goals, company policy, and/or community standards may influence the quality of manually implemented test suites, automatically generated ones are similiarly affected by both the choice of the tool and the configuration of the tool's parameters.  Test suite quality is frequently measured based on the amount of code covered and on the fault-finding ability of the test suite.  Code coverage is a structure-based criterion that requires the exercising of certain control structures and variables within program~\cite{kapfhammer-testing-handbook}. Fault-based test adequacy criterion, on the other hand, attempts to ensure that the program does not contain the types of faults that are commonly introduced into by programmers software systems~\cite{demillo1978hints, zhu1997software}.  

% There is a such thing as coverage and mutation and we will tell you a little about them

A common type of test quality evaluation, in both industrial and academic work, leverages structurally-based criteria that require the execution of a statement or branch in a program~\cite{weyuker1988evaluation}.  Alternatively, mutation testing is a fault-based technique that measures the fault-finding effectiveness of test suites on the basis of induced faults~\cite{demillo1978hints, hamlet1977testing}.  Originally proposed by DeMillo et al.~\cite{demillo1978hints}, mutation testing is a well-known technique that evaluates the quality of tests by seeding faults into the program under test; each altered version containing a seeded fault is called a mutant. When the results of a test case vary if it is executed against the mutant instead of the original program, the mutant is said to be killed and the ratio of killed mutants to generated mutants is known as the higher-is-better mutation score. In mutation testing, in a conditional statement such as \texttt{if (a < b)}, the \texttt{<} sign will be replaced with all other possible relational operators such as \texttt{>, <=, >= , ==}, and \texttt{!=}. Previous studies have used mutation adequacy to experimentally gauge the effectiveness of different testing strategies~\cite{andrews2005mutation,andrews2006,do2006,just2014}.  

% What we do

When confronted with the wide variety of non-standardized manual testing strategies and complex automated test generation tools, real-world software developers face the challenge of defining a strategy for developing quality software.  Through an empirical study of automatically and manually generated test suites for ten real-world open-source programs (e.g., NetWeaver and JSecurity), this paper characterizes and compares the tests, providing useful insights for both researchers and practitioners. With a focus on the time required to automatically create the test suite and the statement, branch, and mutation scores of the tests, the experiments compare the manually created tests with those that are automatically generated by two automated tools, CodePro and \textsc{EvoSuite}.  

% What we do

Although automatic test generation requires far less human time and effort than manual test generation, this time savings may not be worthwhile if the quality of the resulting test suites is poor in terms of coverage or fault-finding capability.  Alternatively, automatically generated tests may not be as useful as manually implemented ones if they are too complex or difficult to maintain.  So, this paper additionally examines the complexity and size of the original program and both automatically and manually generated test.  Finally, we point out the practical benefits and challenges associated with using automatically generated tests instead of manually created ones.

% Compare to the more related paper and then set the state for our empirical results

While it is true that this paper is similar to an experimental study by Bacchelli et al.~\cite{bacchelli2008}, it is important to note that there are key distinctions between our work and this prior paper.  Briefly, the experiments in this paper incorporate ten real-world programs (instead of individual classes from one program), include tests manually implemented by open-source developers (instead of the authors), use state-of-the-art test suite generators (rather than tools known to be less effective), and leverage a mutation testing tool that creates mutants known to be representative of real faults (instead of using a tool with less empirical validation).  These, and other, advantages allow this paper to make the following empirical observations:

% how the code complexity and lines of code in the original program relate to these same metrics for the test suites and
% the number of test cases generated.  Finally, we discuss the overall impact of using automated tools instead of manual
% test creation.

% Evaluation results

% Summary of the important contributions
In summary, the main contributions of this paper are as follows:
\squishlist 
\item An examination of the techniques used in sophisticated automatic test case generation tools.
\item An empirical analysis of existing manually written test suites for open source applications.
\item An empirical analysis of automatically generated test suites for open source applications.
\item A comparison of manually written test suites and automatically generated test suites.
\item A discussion of the benefits and drawbacks of using automatic test case generation tools.
\squishend 

