%!TEX root=qsic2014.tex
% mainfile: qsic2014.tex

\section{Introduction}

% Software testing is a vital task throughout the software development lifecycle for helping to create quality software.

Since the release of low quality software has serious economic and social consequences~\cite{tassey2002}, software testing is commonly used throughout the software development lifecycle to identify defects and establish a confidence in program correctness. In fact, Kochhar et al.'s recent investigation of over 20,000 GitHub projects revealed that over 61\% of them include test cases~\cite{kochhar2013}.  However, even though testing is both valuable and prevalent, the creation, execution, and maintenance of tests is also one of the most expensive aspects of developing software, often comprising atleast 25\% of the total budget for an information technology department~\cite{vizard2013}.  

% Manual and automated testing, and the lack of clarity about the quality of test suites

Traditionally, test cases are written manually by software developers and/or members of a quality assurance team.  Unfortunately, manual creation of a high quality test suite requires substantial amounts of human thought, time, and effort.  Although manual creation gives developers control over what code they test and how thoroughly they test it, the time and effort required to produce these test suites can be prohibitively high in large projects that often necessitate the most test cases~\cite{kochhar2013}.  As an alternative to manually writing test cases, many automatic test case generation tools (e.g.,~\cite{fraser:2011:eat:2025113.2025179,pacheco2007feedback,csallner2004}) are now available to help developers test their software.  However, the quality of the test suites produced by these tools varies~\cite{bacchelli2008,fraser2013c,fraser2013a}, and it is unclear how the test suites of automatic test generation tools compare to those that are manually developed.

% Manual Generated Tests -  benefits vs drawbacks

When given sufficient time and resources, developers can often manually implement a high quality test suite that exercises the majority of the application's features and covers its source code.  Yet, the focus of the test cases may vary depending on the software developer, the goals of the project, and/or the standards of the company or open-source community~\cite{kochhar2013}.  Some developers may write tests with the goal of increasing the coverage of the code, particularly in terms of statements or branches.  Others may focus tests on code that is likely to fail, most commonly executed, or ``important'' according to other standards~\cite{mockus2009}. In any case, as the size and complexity of a program increases, manually writing test cases can become expensive, thus requiring additional human thought and labor when applied to highly complex features~\cite{clarke1998automated}.

% The lack of standards for test suites

% Talk about auto generated test suites - benefits vs drawbacks

Although there are tutorials that explain how to write JUnit test suites (e.g., \cite{vogella2013}), there is no well-established standard for writing test cases, thus making the testing process even more challenging for developers.  Instead of, or in addition to, manually implementing test cases without rigorous guidelines, developers may employ automatic test case generation tools that could reduce the time and cost associated with testing while also improving code coverage.  Many automatic test suite generators currently exist.  Some tools, like the MoreUnit plugin for Eclipse \cite{moreunit}, generate test case stubs for the method that is currently highlighted by the cursor.  Alternatively, tools such as CodePro AnalytiX~\cite{codepro1}, \textsc{EvoSuite}~\cite{fraser:2011:eat:2025113.2025179}, JCrasher~\cite{csallner2004}, Palus~\cite{zhang:2011:pha:1985793.1986036}, Randoop~\cite{pacheco2007feedback}, and TestEra~\cite{marinov:2001:tnf:872023.872551} can generate complete test suites that need no modification prior to execution.

% Compare Automated vs Manual

% This has to be connected to empirical studies that compare both automated and manual?

% While goals, company policy, and/or community standards may influence a developer's choice to test certain
% code segments, the ability of test suites to find bugs is not frequently analyzed.

Automatic test case generation tools use both deterministic (e.g., hard-coded rules and regular expressions) and
learning-based (e.g., randomized or evolutionary) algorithms to produce test cases based on particular strategies,
thereby avoiding the subjectivity and wide variation in styles commonly found in manually generated tests.  Test suite
generators also have the potential to significantly reduce the amount of human time and effort required of the developer
to create the test suite.  

% However, the quality of the test suites is also of vital concern.  

While goals, company policy, and/or community standards may influence the quality of manually implemented test suites, automatically generated ones are similiarly affected by both the choice of the tool and the configuration of the tool's parameters.  Test suite quality is frequently measured based on the amount of code covered and on the fault-finding ability of the test suite.  Code coverage is a structure-based criterion that requires the exercising of certain control structures and variables within program~\cite{kapfhammer-testing-handbook}. Fault-based test adequacy criterion, on the other hand, attempts to ensure that the program does not contain the types of faults that are commonly introduced into by programmers software systems~\cite{demillo1978hints, zhu1997software}.  

% There is a such thing as coverage and mutation and we will tell you a little about them

A common type of test quality evaluation, in both industrial and academic work, leverages structurally-based criteria that require the execution of a statement or branch in a program~\cite{weyuker1988evaluation}.  Alternatively, mutation testing is a fault-based technique that measures the fault-finding effectiveness of test suites on the basis of induced faults~\cite{demillo1978hints, hamlet1977testing}.  Originally proposed by DeMillo et al.~\cite{demillo1978hints}, mutation testing is a well-known technique that evaluates the quality of tests by seeding faults into the program under test; each altered version containing a seeded fault is called a mutant. When the results of a test case vary if it is executed against the mutant instead of the original program, the mutant is said to be killed and the ratio of killed mutants to generated mutants is known as the higher-is-better mutation score. In mutation testing, in a simple statement such as \texttt{if (a < b)}, the \texttt{<} sign will be replaced with all other possible relational operators such as \texttt{>, <=, >= , ==, !=}. Previous studies have used mutation adequacy to experimentally gauge the effectiveness of different testing strategies~\cite{andrews2005mutation,andrews2006,do2006,just2014}.  

% What we do

In this paper, we analyze existing manually written test suites of open source applications and compare these to test
suites that are generated using a two automatic test case generation tools, CodePro and \textsc{EvoSuite}, to identify
benefits and drawbacks of using automated test suite generation tools.  The three sets of test suites are compared based
on the time required to create the test suite, the statement and branch coverage of each test suite, and the mutation
score of each test suite.  Although automatic test generation requires far less time and effort than manual test
generation, this improvement may not be worthwhile if the quality of the resulting test suites is low in terms of
coverage or fault-finding ability.  We additionally examine how the code complexity and lines of code in the original
program relate to the code complexity and the lines of code in the tests and the number of test cases generated.
Finally, we discuss the overall impact of using automated tools instead of manual test creation.

%Evaluation results
The results indicate stuff.

%contributions
In summary, the main contributions of this paper are as follows:
\squishlist 
\item An examination of the techniques used in sophisticated automatic test case generation tools.
\item An empirical analysis of existing manually written test suites for open source applications.
\item An empirical analysis of automatically generated test suites for open source applications.
\item A comparison of manually written test suites and automatically generated test suites.
\item A discussion of the benefits and drawbacks of using automatic test case generation tools.
\squishend 

