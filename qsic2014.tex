%!TEX root=qsic2014.tex
% mainfile: qsic2014.tex

% Jeshua Kracht and Jacob Petrovic

\documentclass[conference]{IEEEtran}
%\makeindex
\usepackage{cite}
\usepackage[hidelinks]{hyperref}
\usepackage[cmex10]{amsmath}
\usepackage{url}
\usepackage{mathtools}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{color}
\usepackage{xspace}
\usepackage{booktabs}

\newcommand{\squishlist}{
 \begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{3pt}
     \setlength{\topsep}{3pt}
     \setlength{\partopsep}{0pt}
     \setlength{\leftmargin}{1.5em}
     \setlength{\labelwidth}{1em}
     \setlength{\labelsep}{0.5em} } }

\newcommand{\squishlisttwo}{
 \begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{0pt}
    \setlength{\topsep}{0pt}
    \setlength{\partopsep}{0pt}
    \setlength{\leftmargin}{2em}
    \setlength{\labelwidth}{1.5em}
    \setlength{\labelsep}{0.5em} } }

\newcommand{\squishend}{
  \end{list}  }


% Comment out one of these two definitions to show/hide comments in the paper
\newcommand{\todo}[1]{\relax}
%\newcommand{\todo}[1]{{\color{red}\bfseries [[#1]]}}

%% Macros for tool names
\def\evo{\textsc{EvoSuite}\xspace}
\def\codepro{CodePro\xspace}
\def\jacoco{Jacoco\xspace}
\def\randoop{Randoop\xspace}
\def\jcrasher{JCrasher\xspace}
\def\palus{Palus\xspace}
\def\testera{TestEra\xspace}

%% Macros for subject programs
\def\netweaver{Netweaver\xspace}
\def\inspirento{Inspirento\xspace}
\def\jsecurity{Jsecurity\xspace}
\def\saxpath{Saxpath\xspace}
\def\jni{Jni-inchi\xspace}
\def\xisemele{Xisemele\xspace}
\def\diebierse{Diebierse\xspace}
\def\lagoon{Lagoon\xspace}
\def\lavalamp{Lavalamp\xspace}
\def\jnfe{Jnfe\xspace}

%% Reduce arraystretch in tables if we need to save space
\renewcommand{\arraystretch}{1.025}

\begin{document}
\title{Empirically Evaluating the Quality of Automatically Generated and Manually Written Test Suites}
% \subtitle{[Extended Abstract]}

\author{\IEEEauthorblockN{Jeshua Kracht, Jacob Z. Petrovic, Kristen R. Walcott-Justice}
\IEEEauthorblockA{Department of Computer Science\\
University of Colorado at Colorado Springs\\
Email: \{jkracht, jpetrovi, kjustice\}@uccs.edu}
%\and
%\IEEEauthorblockN{Jacob Z. Petrovic}
%\IEEEauthorblockA{Department of Computer Science\\
%University of Colorado at Colorado Springs\\
%Email: jpetrovi@uccs.edu}
%\and
%\IEEEauthorblockN{Kristen R. Walcott-Justice}
%\IEEEauthorblockA{Department of Computer Science\\
%University of Colorado at Colorado Springs\\
%Email: kjustice@uccs.edu}
\and
\IEEEauthorblockN{Gregory M. Kapfhammer}
\IEEEauthorblockA{Department of Computer Science\\
Allegheny College\\
Email: gkapfham@allegheny.edu}
}

\maketitle
\begin{abstract}
\todo{Define the term quality of test cases in abstract! Quality in this paper 
refers to complexity, efficiency, and effectiveness.}

\todo{Make sure we use a consistent terminology throughout the paper -- this
terminology should also align with the title: 

test cases vs tests vs test suite

developer-written vs. manually-written test cases}

  The creation, execution, and maintenance of tests are some of the most expensive tasks in software development. To help reduce the cost, automated test generation tools can be used to assist and guide developers in creating test cases. Yet, the tests that automated tools produce range from simple skeletons to fully executable test suites, and hence their complexity and quality vary.
  
  This paper compares the complexity and quality of test suites created by sophisticated automated test generation tools to that of developer-written test suites. The empirical study in this paper examines ten real-world\todo{Some of the programs are rather toy programs -- still keep real-world?} programs with existing test suites and applies two state-of-the-art automated test generation tools. The study measures the resulting test suite quality in terms of code coverage and fault-finding capability. On average, manual tests covered 31.5\% of the branches while the automated tools covered 31.8\% of the branches.\todo{The difference is vanishingly small, and I am not sure what the takeaway should be.}\todo{Can we provide the simpler metric statement coverage -- overall, I think we need more data here: Do the tests cover the same code?} In terms of mutation score\todo{This term has not been defined yet and the abstract does not say anything about mutation analysis!}\todo{Is this the mutation score w.r.t. covered or all mutants?}, the tests generated by automated tools had an average mutation score of 39.8\% compared to the average mutation score of 42.1\% for manually written tests.  Even though automatically created tests often contain more lines of source code than those written by developers, this paper's empirical results reveal that test generation tools can provide value by creating high quality\todo{Considering the low coverage and mutation score, this claim about absolute quality is odd. We might, however, comment on the quality compared to the developer-written suites.} test suites while reducing the cost and effort needed for testing\todo{Cost and effort is not measured in the paper -- we need to rephrase this statement.}.  

\end{abstract}

\input{introduction_v2}
\input{background_v3}
\input{evaluation_v2}
\input{related_work}
\input{conclusion}

\section{Acknowledgements}                                                       
We thank Ren\'e Just for his comments on earlier versions of this paper and for his support with the MAJOR system.

\vspace*{-.125in}

\bibliographystyle{IEEEtran}
\bibliography{sample}
%\balancecolumns 

\end{document}
