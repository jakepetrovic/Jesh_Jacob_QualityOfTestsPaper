%!TEX root=qsic2014.tex
% mainfile: qsic2014.tex

% Jeshua Kracht and Jacob Petrovic

\documentclass[conference]{IEEEtran}
%\makeindex
\usepackage{cite}
\usepackage[hidelinks]{hyperref}
\usepackage[cmex10]{amsmath}
\usepackage{url}
\usepackage{mathtools}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{color}
\usepackage{xspace}
\usepackage{booktabs}

\newcommand{\squishlist}{
 \begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{3pt}
     \setlength{\topsep}{3pt}
     \setlength{\partopsep}{0pt}
     \setlength{\leftmargin}{1.5em}
     \setlength{\labelwidth}{1em}
     \setlength{\labelsep}{0.5em} } }

\newcommand{\squishlisttwo}{
 \begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{0pt}
    \setlength{\topsep}{0pt}
    \setlength{\partopsep}{0pt}
    \setlength{\leftmargin}{2em}
    \setlength{\labelwidth}{1.5em}
    \setlength{\labelsep}{0.5em} } }

\newcommand{\squishend}{
  \end{list}  }


% Comment out one of these two definitions to show/hide comments in the paper
%\newcommand{\todo}[1]{\relax}
\newcommand{\todo}[1]{{\color{red}\bfseries [[#1]]}}

%% Macros for tool names
\def\evo{\textsc{EvoSuite}\xspace}
\def\codepro{CodePro\todo{RJ: Should this be CodePro AnalytiX?}\xspace}
\def\jacoco{Jacoco\xspace}
\def\randoop{Randoop\xspace}
\def\jcrasher{JCrasher\xspace}
\def\palus{Palus\xspace}
\def\testera{TestEra\xspace}

%% Macros for subject programs
\def\netweaver{Netweaver\xspace}
\def\inspirento{Inspirento\xspace}
\def\jsecurity{Jsecurity\xspace}
\def\saxpath{Saxpath\xspace}
\def\jni{Jni-inchi\xspace}
\def\xisemele{Xisemele\xspace}
\def\diebierse{Diebierse\xspace}
\def\lagoon{Lagoon\xspace}
\def\lavalamp{Lavalamp\xspace}
\def\jnfe{Jnfe\xspace}

%% Reduce arraystretch in tables if we need to save space
\renewcommand{\arraystretch}{1.025}

\begin{document}
\title{Empirically Evaluating the Quality of Automatically Generated and Manually Written Test Suites}
% \subtitle{[Extended Abstract]}

\author{\IEEEauthorblockN{Jeshua Kracht, Jacob Z. Petrovic, \\Kristen R. Walcott-Justice}
\IEEEauthorblockA{Department of Computer Science\\
University of Colorado at Colorado Springs\\
Email: \{jkracht, jpetrovi, kjustice\}@uccs.edu}
%\and
%\IEEEauthorblockN{Jacob Z. Petrovic}
%\IEEEauthorblockA{Department of Computer Science\\
%University of Colorado at Colorado Springs\\
%Email: jpetrovi@uccs.edu}
%\and
%\IEEEauthorblockN{Kristen R. Walcott-Justice}
%\IEEEauthorblockA{Department of Computer Science\\
%University of Colorado at Colorado Springs\\
%Email: kjustice@uccs.edu}
\and
\IEEEauthorblockN{Ren\'{e} Just}
\IEEEauthorblockA{Computer Science \& Engineering\\
University of Washington\\
Email: rjust@cs.washington.edu}
\and
\IEEEauthorblockN{Gregory M. Kapfhammer}
\IEEEauthorblockA{Department of Computer Science\\
Allegheny College\\
Email: gkapfham@allegheny.edu}
}

\maketitle
\begin{abstract}

  The creation, execution, and maintenance of tests are some of the most expensive tasks in software development. To help reduce the cost, automated test generation tools have been created to assist and guide developers in writing test cases. Yet, the quality of the test that automated tools produce can be limited, ranging from simple skeletons to fully executable \mbox{test suites}.  
  
  In this paper, the quality of test suites created by sophisticated automated test case generation tools is compared to that of test suites that were manually written by developers. We examine ten real-world programs (e.g., \netweaver and \jsecurity) with existing test suites, apply two state-of-the-art automated test generation tools, and analyze the resulting test suites in terms of code coverage and overall fault-finding ability, ultimately comparing to the manually written tests. On average, manual tests covered 31.5\% of the branches while the automated tools covered 31.8\% of the branches. In terms of mutation score, the tests generated by automated tools had an average mutation score of 39.8\% compared to the average mutation score of 42.1\% for manually written tests.  Even though automatically created tests often contain more lines of source code than those written by developers, this paper's empirical results reveal that test generation tools can provide value by creating high quality test suites while reducing the cost and effort needed for testing.  

\end{abstract}

\input{introduction_v2}
\input{background_v3}
\input{evaluation_v2}
\input{related_work}
\input{conclusion}

\vspace*{-.125in}

\bibliographystyle{IEEEtran}
\bibliography{sample}
%\balancecolumns 

\end{document}
